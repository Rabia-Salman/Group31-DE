# -*- coding: utf-8 -*-
"""reddit_scrapper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qhdOogXgOTmh1pLvU4MreRBWVn5kg1l8
"""
!pip install asyncpraw pandas nest_asyncio
!pip install vadersentiment
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime 
import requests
import praw
import asyncpraw
import asyncio
import nest_asyncio
!pip install wordcloud nltk
from collections import Counter
from wordcloud import WordCloud
import nltk
from nltk.corpus import stopwords

!pip install pytrends

CLIENT_ID = 'ffm_9E8Ca89J2WxhWxbihQ'
CLIENT_SECRET = 'pDp8znm5JCZjLjNUPgse4nwjVke2nA'
USER_AGENT = 'Reddit-crawler'

reddit = praw.Reddit(
    client_id=CLIENT_ID,
    client_secret=CLIENT_SECRET,
    user_agent="sports_match_scraper:v1.0 (by u/your_reddit_username)"
)

subreddits = ["cricket", "soccer", "hockey"]

posts = []
for subreddit_name in subreddits:
    subreddit = reddit.subreddit(subreddit_name)
    print(f"Fetching posts from r/{subreddit_name}")
    # for post in subreddit.search("Cricket", limit=100):  # Search discussions
    for post in subreddit.hot(limit=100):  # Fetch top 100 posts
        posts.append([
            subreddit_name,
            post.title,
            post.downs,
            post.score,
            post.num_comments,
            post.url,
            post.ups,
            post.upvote_ratio,
            post.created_utc
        ])

df = pd.DataFrame(posts, columns=["Subreddit", "Title","Downs", "Score", "Comments", "URL", "Ups", "Upvote_Ratio", "Timestamp"])
df.to_csv("sports_match_discussions.csv", index=False, encoding="utf-8")

print("CSV file saved: sports_match_discussions.csv")

!kaggle datasets download -d rush4ratio/cricsheet-match-data

df = pd.read_csv("sports_match_discussions.csv")

df["Timestamp"] = pd.to_datetime(df["Timestamp"], unit="s")

df["Hour"] = df["Timestamp"].dt.hour
df["Day"] = df["Timestamp"].dt.day_name()
df["Month"] = df["Timestamp"].dt.month_name()

print(df.head())  # Preview data

# Top 10 posts by Score
top_posts = df.nlargest(10, "Score")
print(top_posts[["Title", "Score", "Subreddit"]])
top_commented_posts = df.nlargest(10, "Comments")
print(top_commented_posts[["Title", "Comments", "Subreddit"]])

plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x="Ups", y="Comments", hue="Subreddit", alpha=0.7)
plt.xlabel("Upvotes")
plt.ylabel("Number of Comments")
plt.ylim(0, 5000)
plt.title("Upvotes vs. Comments on Sports Discussions")
plt.legend(title="Subreddit")
plt.show()

hourly_scores = df.groupby("Hour")["Score"].mean()

plt.figure(figsize=(10, 5))
sns.lineplot(x=hourly_scores.index, y=hourly_scores.values, marker="o")
plt.xlabel("Hour of the Day")
plt.ylabel("Average Score")
plt.title("Best Time to Post Based on Score")
plt.xticks(range(0, 24))
plt.show()


analyzer = SentimentIntensityAnalyzer()
df["Sentiment"] = df["Title"].apply(lambda x: analyzer.polarity_scores(str(x))["compound"])

plt.figure(figsize=(10, 5))
sns.histplot(df["Sentiment"], bins=20, kde=True)
plt.xlabel("Sentiment Score")
plt.ylabel("Number of Posts")
plt.title("Sentiment Analysis of Sports Discussions")
plt.show()

nltk.download("stopwords")
stop_words = set(stopwords.words("english"))
text = " ".join(df["Title"].astype(str))

words = [word for word in text.split() if word.lower() not in stop_words]
filtered_text = " ".join(words)

wordcloud = WordCloud(width=800, height=400, background_color="white", colormap="coolwarm").generate(filtered_text)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.title("Common Words in Sports Discussions")
plt.show()

nltk.download("stopwords")
stop_words = set(stopwords.words("english"))

all_words = " ".join(df["Title"].astype(str)).lower().split()
filtered_words = [word for word in all_words if word not in stop_words]
word_counts = Counter(filtered_words)
top_keywords = [word for word, _ in word_counts.most_common(5)]  # Top 5 keywords
print("Top Reddit Keywords:", top_keywords)



